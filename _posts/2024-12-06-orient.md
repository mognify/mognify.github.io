---
layout: post
title: Orient
date: 2024-12-06 07:11 -0600
description: Ghost in the machine
categories: [Software, Tools]
tags: [tool, video, software]
pin: true
---
My #1 ambitious project right now is to make a program with LLM, ML, voice transcription (for training to record the sound of whoever is instructing me on how to do my job, i.e. in training; for performing to analyze the sound of whoever is asking me a question), LLM (to analyze the transcriptions and understand what im saying), ML (to look back on the trained answer/behavior), voice synthesis (TTS that recreates my voice to answer the question or prompt the user), and Qt (to display highlights showing me where it predicts i'll click, and what it predict's i'll type)...

---

When you start a typical remote tech support job, typically you will go through 1-4 weeks of structured training.

The instructor will show you slides, provide you with guide documents, and either just show you what to do on each page of their internal UIs, or have you follow along (ideally you'll follow along)

This tool should basically have it to where all the stuff they're doing to train you is actually training this offline, local ML to either perform your duties or guide you on what to action (click, type) or what to say (text prompt overlayed at top right of screen in a transparent Qt GUI window)

- Train the voice synthesis to recreate your voice, or just record soundboards of your voice reacting to different prompts (and this way the LLM will just match the transcribed end-user dialogue to one of your soundboard options)
- Train the ML to predict what you'll click on and type based on what it sees (template matching, with percent confidence levels) and "hears" (transcribed audio interpreted by LLM)... and it will either just highlight the areas, or click for you (with consent?)
